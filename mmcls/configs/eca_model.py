_base_ = ["../datasets/cifar.py"]

model = dict(
    type="EcaImClassifier",  # Classifier name
    backbone=dict(
        type="Eca_Resnet_cifar",  # Backbones name
        depth=50,  # depth of backbone, ResNet has options of 18, 34, 50, 101, 152.
        num_stages=4,  # number of stages，The feature maps generated by these states are used as the input for the subsequent neck and head.
        out_indices=(3,),  # The output index of the output feature maps.
        frozen_stages=-1,  # the stage to be frozen, '-1' means not be forzen
        style="pytorch",
    ),  # The style of backbone, 'pytorch' means that stride 2 layers are in 3x3 conv, 'caffe' means stride 2 layers are in 1x1 convs.
    neck=dict(type="GlobalAveragePooling"),  # neck network name
    head=dict(
        type="LinearClsHead",  # linear classification head，
        num_classes=10,  # The number of output categories, consistent with the number of categories in the dataset
        in_channels=2048,  # The number of input channels, consistent with the output channel of the neck
        loss=dict(
            type="CrossEntropyLoss", loss_weight=1.0
        ),  # Loss function configuration information
        topk=(
            1,
            5,
        ),  # Evaluation index, Top-k accuracy rate, here is the accuracy rate of top1 and top5
    ),
)

# dataset settings
dataset_type = "CIFAR10"  # dataset name，
img_norm_cfg = dict(  # Image normalization config to normalize the input images
    mean=[
        123.675,
        116.28,
        103.53,
    ],  # Mean values used to pre-training the pre-trained backbone models
    std=[
        58.395,
        57.12,
        57.375,
    ],  # Standard variance used to pre-training the pre-trained backbone models
    to_rgb=True,
)  # Whether to invert the color channel, rgb2bgr or bgr2rgb.
# train data pipeline
train_pipeline = [
    dict(type="LoadImageFromFile"),  # First pipeline to load images from file path
    dict(type="RandomResizedCrop", size=224),  # RandomResizedCrop
    dict(
        type="RandomFlip", flip_prob=0.5, direction="horizontal"
    ),  # Randomly flip the picture horizontally with a probability of 0.5
    dict(type="Normalize", **img_norm_cfg),  # normalization
    dict(
        type="ImageToTensor", keys=["img"]
    ),  # convert image from numpy into torch.Tensor
    dict(type="ToTensor", keys=["gt_label"]),  # convert gt_label into torch.Tensor
    dict(
        type="Collect", keys=["img", "gt_label"]
    ),  # Pipeline that decides which keys in the data should be passed to the detector
]
# test data pipeline
test_pipeline = [
    dict(type="LoadImageFromFile"),
    dict(type="Resize", size=(256, -1)),
    dict(type="CenterCrop", crop_size=224),
    dict(type="Normalize", **img_norm_cfg),
    dict(type="ImageToTensor", keys=["img"]),
    dict(type="Collect", keys=["img"]),  # do not pass gt_label while testing
]

# he configuration file used to build the optimizer, support all optimizers in PyTorch.
optimizer = dict(
    type="SGD",  # Optimizer type
    lr=0.1,  # Learning rate of optimizers, see detail usages of the parameters in the documentation of PyTorch
    momentum=0.9,  # Momentum
    weight_decay=0.0001,
)  # Weight decay of SGD
# Config used to build the optimizer hook, refer to https://github.com/open-mmlab/mmcv/blob/master/mmcv/runner/hooks/optimizer.py#L8 for implementation details.
optimizer_config = dict(grad_clip=None)  # Most of the methods do not use gradient clip
# Learning rate scheduler config used to register LrUpdater hook
lr_config = dict(
    policy="step",  # The policy of scheduler, also support CosineAnnealing, Cyclic, etc. Refer to details of supported LrUpdater from https://github.com/open-mmlab/mmcv/blob/master/mmcv/runner/hooks/lr_updater.py#L9.
    step=[5, 10, 15],
)  # Steps to decay the learning rate
runner = dict(
    type="EpochBasedRunner",  # Type of runner to use (i.e. IterBasedRunner or EpochBasedRunner)
    max_epochs=20,
)  # Runner that runs the workflow in total max_epochs. For IterBasedRunner use `max_iters`

# Config to set the checkpoint hook, Refer to https://github.com/open-mmlab/mmcv/blob/master/mmcv/runner/hooks/checkpoint.py for implementation.
checkpoint_config = dict(interval=5)  # The save interval is 1
# config to register logger hook
log_config = dict(
    interval=100,  # Interval to print the log
    hooks=[
        dict(type="TextLoggerHook"),  # The Tensorboard logger is also supported
        # dict(type='TensorboardLoggerHook')
    ],
)

log_level = "INFO"  # The output level of the log.
resume_from = None  # Resume checkpoints from a given path, the training will be resumed from the epoch when the checkpoint's is saved.
workflow = [
    ("train", 1)
]  # Workflow for runner. [('train', 1)] means there is only one workflow and the workflow named 'train' is executed once.
work_dir = "work_dir"  # Directory to save the model checkpoints and logs for the current experiments.
